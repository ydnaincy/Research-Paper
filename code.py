# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T4lLAmY-vyPtD7INsR_OyUSwhAlHpo3w
"""

# Create Kaggle config directory
!mkdir ~/.kaggle

# Move the kaggle.json file into the config directory
!cp kaggle.json ~/.kaggle/

# Secure the API key file
!chmod 600 ~/.kaggle/kaggle.json

# Install Kaggle CLI (if not already installed)
!pip install kaggle

# Download the dataset
!kaggle datasets download anikannal/solar-power-generation-data

# Unzip the dataset
!unzip solar-power-generation-data.zip

# Check downloaded files
!ls -l

import pandas as pd

# Load generation data
df_gen = pd.read_csv('Plant_1_Generation_Data.csv')

# Display Generation Data sample
print("=== Generation Data Sample ===")
print(df_gen.head())

# Information about Generation Data
print("\nGeneration Data Info:")
print(df_gen.info())

# Check for missing values in generation data
print("\nMissing Values in Generation Data:")
print(df_gen.isnull().sum())

# Load weather data
df_weather = pd.read_csv('Plant_1_Weather_Sensor_Data.csv')

# Explore weather data
print("\nWeather Data Sample:")
print(df_weather.head())

print("\nWeather Data Info:")
print(df_weather.info())

# Check for missing values in weather data
print("\nMissing Values in Weather Data:")
print(df_weather.isnull().sum())

# Convert DATE_TIME columns to datetime format
df_gen['DATE_TIME'] = pd.to_datetime(df_gen['DATE_TIME'], format='%d-%m-%Y %H:%M')
df_weather['DATE_TIME'] = pd.to_datetime(df_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')

# Verify conversion
print(df_gen['DATE_TIME'].dtype, df_weather['DATE_TIME'].dtype)

# Merge datasets on DATE_TIME and PLANT_ID
df_merged = pd.merge(df_gen, df_weather, on=['DATE_TIME', 'PLANT_ID'], how='inner')

# Check merged dataset
print("Merged Data Sample:")
print(df_merged.head())

# Check size and missing values after merging
print("\nMerged Data Info:")
print(df_merged.info())
print("\nMissing values:")
print(df_merged.isnull().sum())

# Extract useful time-based features
df_merged['HOUR'] = df_merged['DATE_TIME'].dt.hour
df_merged['DAY'] = df_merged['DATE_TIME'].dt.day
df_merged['MONTH'] = df_merged['DATE_TIME'].dt.month
df_merged['DAY_OF_WEEK'] = df_merged['DATE_TIME'].dt.dayofweek

# Verify added features
print(df_merged[['DATE_TIME', 'HOUR', 'DAY', 'MONTH', 'DAY_OF_WEEK']].head())

# Aggregate hourly data
hourly_data = df_merged.groupby(['DATE_TIME', 'HOUR', 'DAY', 'MONTH', 'DAY_OF_WEEK']).agg({
    'DC_POWER': 'mean',
    'AC_POWER': 'mean',
    'DAILY_YIELD': 'mean',
    'TOTAL_YIELD': 'mean',
    'AMBIENT_TEMPERATURE': 'mean',
    'MODULE_TEMPERATURE': 'mean',
    'IRRADIATION': 'mean'
}).reset_index()

# Check aggregated data
print(hourly_data.head())
print("\nAggregated Data Shape:", hourly_data.shape)

from sklearn.preprocessing import MinMaxScaler

# Columns to scale
cols_to_scale = ['DC_POWER', 'AC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD',
                 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']

scaler = MinMaxScaler()
hourly_data[cols_to_scale] = scaler.fit_transform(hourly_data[cols_to_scale])

# Verify scaled data
print("\nScaled Data Preview:")
print(hourly_data.head())

import numpy as np

# Select relevant features
features = ['DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD',
            'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION', 'HOUR', 'DAY', 'MONTH', 'DAY_OF_WEEK']

target = 'AC_POWER'

data = hourly_data[features + [target]]

# Define function to create supervised learning sequences
def create_sequences(data, seq_length=24):
    sequences, targets = [], []
    for i in range(len(data) - seq_length):
        sequences.append(data.iloc[i:i+seq_length][features].values)
        targets.append(data.iloc[i+seq_length][target])
    return np.array(sequences), np.array(targets)

# Create sequences
seq_length = 24  # 24-hour historical window
X, y = create_sequences(data, seq_length)

print(f"Input shape (samples, time steps, features): {X.shape}")
print(f"Target shape (samples,): {y.shape}")

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout
from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D
from sklearn.model_selection import train_test_split

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Transformer parameters
d_model = 64
num_heads = 4

# Input layer
input_layer = Input(shape=(X.shape[1], X.shape[2]))

# Transformer Encoder Layer
x = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(input_layer, input_layer)
x = Dropout(0.1)(x)
x = LayerNormalization()(x + input_layer)

# Global pooling
x = GlobalAveragePooling1D()(x)

# Dense layers
x = Dense(64, activation='relu')(x)
x = Dropout(0.1)(x)

# Output layer (forecasting AC power)
output_layer = Dense(1)(x)

# Define and compile model
transformer_model = Model(inputs=input_layer, outputs=output_layer)
transformer_model.compile(loss='mse', optimizer='adam', metrics=['mae'])

# Check model summary
transformer_model.summary()

# Train the model
history = transformer_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=20,  # Adjust based on results
    batch_size=32,
    verbose=1
)





# Evaluate on test data
test_loss, test_mae = transformer_model.evaluate(X_test, y_test)
print(f"\nTest MAE: {test_mae}")

import matplotlib.pyplot as plt

# Plot loss curve
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title('Transformer Model Training Loss')
plt.show()

import networkx as nx
import numpy as np

# Create a graph object
G = nx.Graph()

# Extract unique plant IDs
plants = df_merged['PLANT_ID'].unique()

# Add nodes (solar plants)
for plant in plants:
    G.add_node(plant)

# Define an adjacency rule (connect plants within the same region)
for i in range(len(plants)):
    for j in range(i + 1, len(plants)):
        # Create an edge if the plants belong to the same dataset (or are geographically close)
        if abs(plants[i] - plants[j]) < 100:  # Example proximity condition
            G.add_edge(plants[i], plants[j])

# Visualize the graph
import matplotlib.pyplot as plt
plt.figure(figsize=(6, 6))
nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500)
plt.title("Graph Representation of Solar Plants")
plt.show()

import numpy as np
import seaborn as sns

# Select numerical features for similarity calculation
spatial_features = ['DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']

# Compute correlation matrix (simulated adjacency)
adj_matrix = df_merged[spatial_features].corr().values

# Plot the adjacency matrix
plt.figure(figsize=(8,6))
sns.heatmap(adj_matrix, annot=True, cmap='coolwarm', xticklabels=spatial_features, yticklabels=spatial_features)
plt.title("Simulated Adjacency Matrix for Spatial Learning")
plt.show()

import os

# Define dataset URL (Replace this with the actual URL of the dataset)
dataset_url = "https://github.com/microsoft/solar-farms-mapping/raw/main/data/solar_farms_india_2021.geojson"

# Define the output filename
dataset_filename = "solar_farms_india_2021.geojson"

# Download the dataset
os.system(f"wget {dataset_url} -O {dataset_filename}")

print(f"Dataset downloaded as {dataset_filename}")

import geopandas as gpd

# Load the dataset
dataset_filename = "solar_farms_india_2021.geojson"
gdf = gpd.read_file(dataset_filename)

# Display basic information
print("\n✅ Dataset Loaded Successfully!")

print("\n📌 Dataset Sample:")
print(gdf.head())

print("\n📌 Dataset Info:")
print(gdf.info())

print("\n📌 Available Columns:", gdf.columns)

# Check for missing values
print("\n📌 Missing Values:")
print(gdf.isnull().sum())

import os

if os.path.exists(dataset_filename):
    print("✅ Dataset successfully downloaded!")
else:
    print("❌ Dataset download failed. Check the URL.")

!pip install contextily

import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx  # Add a background map

# Ensure the data is in the correct coordinate reference system (CRS)
gdf = gdf.to_crs(epsg=3857)  # Convert to Web Mercator projection for mapping

# Plot with better visibility
fig, ax = plt.subplots(figsize=(10, 8))
gdf.plot(ax=ax, marker='o', color='red', markersize=10, alpha=0.8, label="Solar Farms")

# Add a basemap
ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)

# Labels and legend
plt.title("Solar Farms in India with Basemap")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend()
plt.show()

import numpy as np
from sklearn.neighbors import kneighbors_graph
import networkx as nx
import matplotlib.pyplot as plt

# Extract latitude & longitude
locations = gdf[['Latitude', 'Longitude']].values

# Define number of neighbors per node (adjust as needed)
num_neighbors = 5

# Create adjacency matrix using KNN (distance-based connections)
adj_matrix = kneighbors_graph(locations, num_neighbors, mode='connectivity', include_self=False).toarray()

# Convert adjacency matrix to graph (Fixed for NetworkX v3+)
graph = nx.from_numpy_array(adj_matrix)  # ✅ Corrected

# Visualize Graph Structure (showing connectivity)
plt.figure(figsize=(8, 6))
nx.draw(graph, node_size=10, edge_color='gray', alpha=0.5)
plt.title("Graph Structure of Solar Farms (Spatial Connectivity)")
plt.show()

# Print basic graph info
print("\n✅ Graph Information:")
print(nx.info(graph))

# Print graph statistics manually (since nx.info() is deprecated)
num_nodes = nx.number_of_nodes(graph)
num_edges = nx.number_of_edges(graph)

print("\n✅ Graph Information:")
print(f"📌 Number of Nodes: {num_nodes}")
print(f"📌 Number of Edges: {num_edges}")
print(f"📌 Average Degree: {num_edges / num_nodes:.2f}")

!pip install torch_geometric torch_sparse torch_scatter torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx

# Convert GeoDataFrame columns to node features (latitude, longitude, area)
node_features = torch.tensor(gdf[['Latitude', 'Longitude', 'Area']].values, dtype=torch.float32)

# Convert NetworkX graph to PyTorch Geometric format
pyg_graph = from_networkx(graph)

# Assign node features
pyg_graph.x = node_features  # Set node attributes

# Print graph details
print("\n✅ PyTorch Geometric Graph Created:")
print(pyg_graph)

# Check if features are correctly assigned
print("\n📌 Node Features Shape:", pyg_graph.x.shape)
print("📌 Edge Index Shape:", pyg_graph.edge_index.shape)

import os

# Create .kaggle directory if not exists
os.makedirs("/root/.kaggle", exist_ok=True)

# Upload the kaggle.json file manually before running this
from google.colab import files
uploaded = files.upload()

# Move kaggle.json to the correct location
os.system("mv kaggle.json /root/.kaggle/")

# Set correct permissions
os.system("chmod 600 /root/.kaggle/kaggle.json")

# Download the dataset from Kaggle
os.system("kaggle datasets download anikannal/solar-power-generation-data")

# Unzip the dataset
os.system("unzip solar-power-generation-data.zip")

print("\n✅ Dataset Downloaded & Extracted Successfully!")

import os

# List files in the current directory
print("\n📂 Available Files in Directory:")
print(os.listdir())

!pip install torch torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

import pandas as pd

# Load Generation Data
df_gen = pd.read_csv("Plant_1_Generation_Data.csv")

# Load Weather Data
df_weather = pd.read_csv("Plant_1_Weather_Sensor_Data.csv")

# Merge Generation & Weather Data
df_combined = pd.merge(df_gen, df_weather, on=["DATE_TIME", "PLANT_ID", "SOURCE_KEY"], how="inner")

# Instead of using Latitude/Longitude, match using PLANT_ID (if available)
df_final = pd.merge(gdf, df_combined, left_on="fid", right_on="PLANT_ID", how="inner")

# Display Merged Data
print("\n✅ Merged Dataset Sample:")
print(df_final.head())

print("\n📌 Final Merged Dataset Info:")
print(df_final.info())

# Check for missing values (Fixing previous TypeError)
print("\n📌 Missing Values (excluding geometry column):")
print(df_final.drop(columns=["geometry"]).isnull().sum())

print("\n📌 Unique PLANT_IDs in Power Generation Data:")
print(df_combined["PLANT_ID"].unique())

print("\n📌 Unique fids in Solar Farms Dataset:")
print(gdf["fid"].unique())

print("\n📌 Sample Coordinates from Solar Farms Data (gdf):")
print(gdf[['Latitude', 'Longitude']].head())

print("\n📌 Sample Coordinates from Power Generation Data (Weather Proxy):")
print(df_combined[['AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE']].head())  # Checking if it matches approx

print("\n📌 Power Generation Data Columns:")
print(df_gen.columns)

print("\n📌 First Few Rows of Power Generation Data:")
print(df_gen.head())

print("\n📌 Weather Data Columns:")
print(df_weather.columns)

print("\n📌 First Few Rows of Weather Data:")
print(df_weather.head())

import numpy as np

# Extract unique PLANT_IDs from power generation dataset
print("\n📌 Unique PLANT_IDs in Power Generation Data:")
print(df_gen["PLANT_ID"].unique())

# Extract sample coordinates from the solar farms dataset
print("\n📌 Sample Coordinates from Solar Farms Data:")
print(gdf[['Latitude', 'Longitude']].head())

# Print approximate coordinates from power generation data (for comparison)
df_combined["Latitude_Rounded"] = np.round(df_combined["AMBIENT_TEMPERATURE"], 2)  # Proxy for location
df_combined["Longitude_Rounded"] = np.round(df_combined["MODULE_TEMPERATURE"], 2)  # Proxy for location

print("\n📌 Sample Coordinates from Power Data (Using Temp Proxies):")
print(df_combined[['Latitude_Rounded', 'Longitude_Rounded']].head())

print("\n📌 Power Generation Data Shape:", df_gen.shape)
print("📌 Weather Data Shape:", df_weather.shape)

print("\n📌 Sample `DATE_TIME` from Power Generation Data:")
print(df_gen["DATE_TIME"].head())

print("\n📌 Sample `DATE_TIME` from Weather Data:")
print(df_weather["DATE_TIME"].head())

df_check = df_gen.merge(df_weather, on=["DATE_TIME", "PLANT_ID", "SOURCE_KEY"], how="inner")
print("\n✅ Matching Rows Before Merging:", df_check.shape[0])

import pandas as pd

# Explicitly convert `DATE_TIME` to ensure full timestamp format
df_gen["DATE_TIME"] = pd.to_datetime(df_gen["DATE_TIME"].astype(str) + " 00:00:00", format="%Y-%m-%d %H:%M:%S")

# Verify Fix
print("\n📌 Re-Fixed `DATE_TIME` Format in `df_gen`:")
print(df_gen["DATE_TIME"].head())

# Attempt merging again
df_merged = pd.merge(df_gen, df_weather, on=["DATE_TIME", "PLANT_ID"], how="inner")

# Check the number of rows in the merged dataframe
print("\n✅ Successfully Merged! Rows:", df_merged.shape[0])

# Display the merged data sample
print("\n📌 Merged Data Sample:")
print(df_merged.head())

# Select relevant features for prediction
features = ["DC_POWER", "AC_POWER", "AMBIENT_TEMPERATURE", "MODULE_TEMPERATURE", "IRRADIATION"]
target = "DAILY_YIELD"

# Normalize the features using MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_merged[features] = scaler.fit_transform(df_merged[features])

# Display Data Sample
print("\n✅ Preprocessed Data Sample:")
print(df_merged[features + [target]].head())

# Check for missing values
print("\n📌 Missing Values:")
print(df_merged[features + [target]].isnull().sum())

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Define the features (X) and target (y)
X = df_merged[features]
y = df_merged[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print performance metrics
print(f"📌 Mean Absolute Error (MAE): {mae}")
print(f"📌 Mean Squared Error (MSE): {mse}")
print(f"📌 Root Mean Squared Error (RMSE): {rmse}")

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor

# Define the model
rf_model = RandomForestRegressor(random_state=42)

# Define hyperparameters for tuning
param_dist = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Set up the randomized search
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist,
                                   n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)

# Fit the random search
random_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = random_search.best_params_
best_score = random_search.best_score_

print(f"Best Parameters: {best_params}")
print(f"Best Cross-Validation Score: {best_score}")

# Evaluate on the test data with the best model
best_rf_model = random_search.best_estimator_
y_pred = best_rf_model.predict(X_test)

# Calculate performance metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_error
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"\n📌 Updated MAE: {mae}")
print(f"📌 Updated MSE: {mse}")
print(f"📌 Updated RMSE: {rmse}")

import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize XGBoost Regressor
xg_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)

# Fit the model
xg_reg.fit(X_train, y_train)

# Make predictions
y_pred_xg = xg_reg.predict(X_test)

# Evaluate performance
mae_xg = mean_absolute_error(y_test, y_pred_xg)
mse_xg = mean_squared_error(y_test, y_pred_xg)
rmse_xg = np.sqrt(mse_xg)

print(f"XGBoost MAE: {mae_xg}")
print(f"XGBoost MSE: {mse_xg}")
print(f"XGBoost RMSE: {rmse_xg}")

# Plot Feature Importance for Random Forest
import matplotlib.pyplot as plt

# Random Forest feature importance
importances = rf_regressor.best_estimator_.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.title("Feature Importance (Random Forest)")
plt.barh(range(X_train.shape[1]), importances[indices], align="center")
plt.yticks(range(X_train.shape[1]), np.array(features)[indices])
plt.xlabel("Relative Importance")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Assuming your data is prepared (X_train, X_test, y_train, y_test)

# Initialize the Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model
rf_regressor.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_regressor.predict(X_test)

# Evaluate performance
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)

print(f"Random Forest MAE: {mae_rf}")
print(f"Random Forest MSE: {mse_rf}")
print(f"Random Forest RMSE: {rmse_rf}")

# Extract feature importance from the trained Random Forest model
importances = rf_regressor.feature_importances_

# Get the indices that would sort the importances in descending order
indices = np.argsort(importances)

# Plot feature importance
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.title("Feature Importance (Random Forest)")
plt.barh(range(len(importances)), importances[indices], align="center")
plt.yticks(range(len(importances)), np.array(features)[indices])  # Assuming 'features' is your list of feature names
plt.xlabel("Relative Importance")
plt.show()

import pandas as pd

# Load Power Generation Data
df_gen = pd.read_csv("Plant_1_Generation_Data.csv")

# Load Weather Data
df_weather = pd.read_csv("Plant_1_Weather_Sensor_Data.csv")

# Convert DATE_TIME columns to datetime format for both datasets
df_gen['DATE_TIME'] = pd.to_datetime(df_gen['DATE_TIME'], format='%d-%m-%Y %H:%M')
df_weather['DATE_TIME'] = pd.to_datetime(df_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')

# Merge both datasets on DATE_TIME and PLANT_ID
df_merged = pd.merge(df_gen, df_weather, on=['DATE_TIME', 'PLANT_ID'], how='inner')

# Verify merged dataset
print("Merged Data Sample:")
print(df_merged.head())

# Check for missing values
print("\nMissing Values in Merged Data:")
print(df_merged.isnull().sum())

from sklearn.preprocessing import MinMaxScaler

# Select numerical columns for scaling
features = ['DC_POWER', 'AC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']

# Initialize MinMaxScaler to scale features
scaler = MinMaxScaler()

# Scale the selected features
df_merged[features] = scaler.fit_transform(df_merged[features])

# Display a preview of the scaled data
print("\n✅ Scaled Data Sample:")
print(df_merged.head())

# Optionally, you can create additional features (like time features):
df_merged['HOUR'] = df_merged['DATE_TIME'].dt.hour
df_merged['DAY'] = df_merged['DATE_TIME'].dt.day
df_merged['MONTH'] = df_merged['DATE_TIME'].dt.month
df_merged['DAY_OF_WEEK'] = df_merged['DATE_TIME'].dt.dayofweek

# Display the new features
print("\n📌 New Time-based Features:")
print(df_merged[['DATE_TIME', 'HOUR', 'DAY', 'MONTH', 'DAY_OF_WEEK']].head())

# Ensure the final dataset is ready for training
print("\n📌 Preprocessed Data Info:")
print(df_merged.info())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Select relevant features and target
features = ['DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD',
            'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION',
            'HOUR', 'DAY', 'MONTH', 'DAY_OF_WEEK']
target = 'AC_POWER'

# Split the data into training and testing sets
X = df_merged[features]
y = df_merged[target]

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Initialize and train the Random Forest model
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

# Predict the values on the test set
y_pred = rf_regressor.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print the evaluation metrics
print(f"📌 MAE: {mae}")
print(f"📌 MSE: {mse}")
print(f"📌 RMSE: {rmse}")

# Feature Importance Analysis
importances = rf_regressor.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.barh(range(len(features)), importances[indices], align="center")
plt.yticks(range(len(features)), [features[i] for i in indices])
plt.xlabel("Feature Importance")
plt.show()

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Define split ratios
split_ratios = [0.3, 0.2, 0.1]  # Corresponds to 70–30, 80–20, 90–10
rmse_scores = []
r2_scores = []

# Loop through each split
for test_ratio in split_ratios:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    rmse_scores.append(rmse)
    r2_scores.append(r2)

# Labels for x-axis
split_labels = ['70–30', '80–20', '90–10']

# Plot RMSE
plt.figure(figsize=(8, 5))
plt.plot(split_labels, rmse_scores, marker='o', label='RMSE')
plt.plot(split_labels, r2_scores, marker='s', label='R² Score')
plt.xlabel("Train–Test Split")
plt.ylabel("Metric Value")
plt.title("Random Forest Performance Across Different Train–Test Splits")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize the Random Forest Regressor
rf = RandomForestRegressor(random_state=42)

# Perform GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_}")

# Get the best model
best_rf = grid_search.best_estimator_

# Predict using the best model
y_pred_best = best_rf.predict(X_test)

# Evaluate the best model
mae_best = mean_absolute_error(y_test, y_pred_best)
mse_best = mean_squared_error(y_test, y_pred_best)
rmse_best = np.sqrt(mse_best)

# Print the evaluation metrics for the best model
print(f"📌 Best MAE: {mae_best}")
print(f"📌 Best MSE: {mse_best}")
print(f"📌 Best RMSE: {rmse_best}")



from sklearn.model_selection import cross_val_score

# Initialize the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Perform cross-validation (using 5-fold cross-validation)
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='neg_mean_squared_error')

# Calculate the mean and standard deviation of the cross-validation scores
mean_cv_score = np.mean(cv_scores)
std_cv_score = np.std(cv_scores)

# Convert negative MSE to positive
mean_cv_score = -mean_cv_score

print(f"📌 Mean Cross-Validation MSE: {mean_cv_score}")
print(f"📌 Cross-Validation Standard Deviation: {std_cv_score}")

mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
print(f"📌 MAPE: {mape}%")

from sklearn.metrics import explained_variance_score
evs = explained_variance_score(y_test, y_pred)
print(f"📌 Explained Variance Score: {evs}")

plt.figure(figsize=(10,6))
plt.plot(y_test.values, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.xlabel('Time')
plt.ylabel('AC_POWER')
plt.legend()
plt.title('Actual vs Predicted AC Power')
plt.show()

residuals = y_test - y_pred
plt.figure(figsize=(10,6))
plt.scatter(y_test, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Actual AC_POWER')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

plt.figure(figsize=(10,6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

importances = rf_regressor.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10,6))
plt.barh(range(len(features)), importances[indices], align="center")
plt.yticks(range(len(features)), [features[i] for i in indices])
plt.xlabel('Feature Importance')
plt.title('Feature Importance for Random Forest Model')
plt.show()

import geopandas as gpd
import torch
import numpy as np
import pandas as pd

# Load the GeoJSON file
gdf = gpd.read_file('/content/solar_farms_india_2021.geojson')

# Inspect the first few rows of the GeoDataFrame
print(gdf.head())

# Extract relevant columns (Latitude and Longitude)
coordinates = gdf[['Latitude', 'Longitude']].values

# Create a DataFrame to use for spatial analysis later
spatial_data = pd.DataFrame(coordinates, columns=['Latitude', 'Longitude'])

# Define a threshold (e.g., 0.01 degrees) to connect nearby solar farms
threshold = 0.01
edges = []

# Create edges based on proximity using latitude and longitude
for i, (lat1, lon1) in enumerate(coordinates):
    for j, (lat2, lon2) in enumerate(coordinates):
        if i != j and np.abs(lat1 - lat2) < threshold and np.abs(lon1 - lon2) < threshold:
            edges.append([i, j])

# Convert edge list to tensor (for PyTorch Geometric)
edges = np.array(edges).T
edge_index = torch.tensor(edges, dtype=torch.long)

# Define node features (Latitude and Longitude)
node_features = torch.tensor(coordinates, dtype=torch.float)

# Create a PyTorch Geometric Data object for the graph
from torch_geometric.data import Data
graph_data = Data(x=node_features, edge_index=edge_index)

# Print some information about the graph
print(f'Graph created with {graph_data.num_nodes} nodes and {graph_data.num_edges} edges.')
print(graph_data)

import networkx as nx
import matplotlib.pyplot as plt

# Convert the PyTorch Geometric Data object to a NetworkX graph
edge_list = graph_data.edge_index.numpy().T
G = nx.Graph()
G.add_edges_from(edge_list)

# Plot the graph
plt.figure(figsize=(10, 8))
nx.draw(G, node_size=10, alpha=0.6, edge_color='orange', with_labels=False)
plt.title("Spatial Connectivity of Solar Farms (Graph Visualization)")
plt.show()

print("Columns in df_merged:")
print(df_merged.columns)

print("\nColumns in gdf:")
print(gdf.columns)

# Merge the spatial data with the time-series data using PLANT_ID and fid
df_merged = df_merged.merge(gdf[['fid', 'Latitude', 'Longitude']], left_on='PLANT_ID', right_on='fid', how='left')

# Now df_merged contains both temporal and spatial features
print(df_merged.head())

print(df_merged['PLANT_ID'].unique())  # Check unique PLANT_IDs
print(gdf['fid'].unique())  # Check unique fid values

print(df_merged['PLANT_ID'].isnull().sum())  # Check for missing PLANT_IDs
print(gdf['fid'].isnull().sum())  # Check for missing fids

0

# Check a few sample values
print(df_merged[['PLANT_ID']].head())
print(gdf[['fid']].head())

from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print(f"📌 R²: {r2}")

import matplotlib.pyplot as plt

# Feature Importance Analysis
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.barh(range(len(features)), importances[indices], align="center")
plt.yticks(range(len(features)), [features[i] for i in indices])
plt.xlabel("Feature Importance")
plt.show()

import geopandas as gpd
from shapely.geometry import Point

# Convert gdf to a GeoDataFrame (if not already)
gdf = gpd.GeoDataFrame(gdf, geometry=gpd.points_from_xy(gdf.Longitude, gdf.Latitude), crs="EPSG:4326")

# Print to confirm conversion
print(gdf.head())

pip install shap

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Redefine input data
features = ['DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION', 'HOUR', 'DAY', 'MONTH', 'DAY_OF_WEEK']
target = 'AC_POWER'

# Preparing the features and target data
X = scaled_data[features].values
y = scaled_data[target].values

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Transformer model parameters
d_model = 64
num_heads = 4

# Define the model architecture
input_layer = Input(shape=(X_train.shape[1],))

# Add MultiHead Attention layer
x = tf.keras.layers.Reshape((X_train.shape[1], 1))(input_layer)  # Adding dimension for attention
x = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
x = Dropout(0.1)(x)
x = LayerNormalization()(x + x)

# Global pooling and Dense layers
x = GlobalAveragePooling1D()(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.1)(x)
output_layer = Dense(1)(x)

# Compile the model
transformer_model = Model(inputs=input_layer, outputs=output_layer)
transformer_model.compile(loss='mse', optimizer='adam', metrics=['mae'])

# Train the model
history = transformer_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Evaluate the model
test_loss, test_mae = transformer_model.evaluate(X_test, y_test)
print(f"Test MAE: {test_mae}")

# Plot the training and validation loss
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title('Transformer Model Training Loss')
plt.show()

# Make predictions
y_pred = transformer_model.predict(X_test)

# Plot the predicted vs actual values
plt.figure(figsize=(10, 6))
plt.plot(y_test, label='Actual Values')
plt.plot(y_pred, label='Predicted Values', linestyle='--')
plt.xlabel('Time Step')
plt.ylabel('AC Power')
plt.legend()
plt.title('Predicted vs Actual AC Power')
plt.show()